{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd875c7757b4d7e89f4db7589bd2714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   9%|8         | 115M/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Final Answer: paris\n",
      "Model Used: DistilBERT\n",
      "Confidence Score: 15.939737319946289\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, \\\n",
    "    BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "distilbert_model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(distilbert_model_name)\n",
    "distilbert_model = DistilBertForQuestionAnswering.from_pretrained(distilbert_model_name)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "bert_model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertForQuestionAnswering.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define a context and a question\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"\"\"Paris is the capital and most populous city of France, with an estimated\n",
    "population of 2,175,601 residents as of 2018, in an area of more than 105 square\n",
    "kilometres. The City of Paris is the centre and seat of government of the region\n",
    "and province of ÃŽle-de-France, or Paris Region, which has an estimated population\n",
    "of 12,174,880, or about 18 percent of the population of France as of 2017.\"\"\"\n",
    "\n",
    "# Function to get answer from DistilBERT\n",
    "def get_distilbert_answer(question, context):\n",
    "    inputs = distilbert_tokenizer(question, context, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = distilbert_model(**inputs)\n",
    "    \n",
    "    start = torch.argmax(outputs.start_logits)\n",
    "    end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    confidence = float(outputs.start_logits[0, start] + outputs.end_logits[0, end])\n",
    "    answer_tokens = inputs.input_ids[0, start:end+1]\n",
    "    answer = distilbert_tokenizer.decode(answer_tokens)\n",
    "    \n",
    "    return answer, confidence\n",
    "\n",
    "# Function to get answer from BERT\n",
    "def get_bert_answer(question, context):\n",
    "    inputs = bert_tokenizer(question, context, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    start = torch.argmax(outputs.start_logits)\n",
    "    end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    confidence = float(outputs.start_logits[0, start] + outputs.end_logits[0, end])\n",
    "    \n",
    "    answer_tokens = inputs.input_ids[0, start:end+1]\n",
    "    answer = bert_tokenizer.decode(answer_tokens)\n",
    "\n",
    "    return answer, confidence\n",
    "\n",
    "# Get answers from both models\n",
    "distilbert_answer, distilbert_confidence = get_distilbert_answer(question, context)\n",
    "bert_answer, bert_confidence = get_bert_answer(question, context)\n",
    "\n",
    "# Simple ensemble: choose the answer with the highest confidence\n",
    "if distilbert_confidence > bert_confidence:\n",
    "    final_answer = distilbert_answer\n",
    "    model_used = \"DistilBERT\"\n",
    "    confidence = distilbert_confidence\n",
    "else:\n",
    "    final_answer = bert_answer\n",
    "    model_used = \"BERT\"\n",
    "    confidence = bert_confidence\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Final Answer: {final_answer}\")\n",
    "print(f\"Model Used: {model_used}\")\n",
    "print(f\"Confidence Score: {confidence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
