{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Best Answer: paris\n",
      "From Window: Paris is the capital and most populous city of France, with an\n",
      "estimated population of 2,175,601 res...\n",
      "Confidence Score: 15.939737319946289\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Define a long context\n",
    "question = \"What is the capital of France?\"\n",
    "long_context = \"\"\"Paris is the capital and most populous city of France, with an\n",
    "estimated population of 2,175,601 residents as of 2018, in an area of more than 105\n",
    "square kilometres. The City of Paris is the centre and seat of government of the\n",
    "region and province of ÃŽle-de-France, or Paris Region, which has an estimated\n",
    "population of 12,174,880, or about 18 percent of the population of France as of\n",
    "2017.\"\"\"\n",
    "\n",
    "def get_answer_sliding_window(question, context, total_len=512, stride=128):\n",
    "    \"\"\"Function to get answer using sliding window\"\"\"\n",
    "    # Tokenize the question and context\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    context_tokens = tokenizer.tokenize(context)\n",
    "\n",
    "    # If the context is short enough, process it directly\n",
    "    if len(question_tokens) + len(context_tokens) + 3 <= total_len:  # +3 for [CLS], [SEP], [SEP]\n",
    "        best_answer, best_score = get_answer(question, context)\n",
    "        return best_answer, best_score, context\n",
    "\n",
    "    # Otherwise, use sliding window\n",
    "    max_question_len = 64  # Limit question length to ensure we have enough space for context\n",
    "    if len(question_tokens) > max_question_len:\n",
    "        question_tokens = question_tokens[:max_question_len]\n",
    "\n",
    "    # Calculate how many tokens we can allocate to the context\n",
    "    max_len = total_len - len(question_tokens) - 3  # -3 for [CLS], [SEP], [SEP]\n",
    "    windows = []\n",
    "    for i in range(0, len(context_tokens), stride):\n",
    "        windows.append(tokenizer.convert_tokens_to_string(context_tokens[i:i+max_len]))\n",
    "        if i + max_len >= len(context_tokens):\n",
    "            break  # Last window\n",
    "\n",
    "    # Get answers from all windows\n",
    "    answers_with_scores = [get_answer(question, window) for window in windows]\n",
    "\n",
    "    # Find the answer with the highest confidence score\n",
    "    best_answer_idx = np.argmax([score for _, score in answers_with_scores])\n",
    "    best_answer, best_score = answers_with_scores[best_answer_idx]\n",
    "    return best_answer, best_score, windows[best_answer_idx]\n",
    "\n",
    "def get_answer(question, context):\n",
    "    \"\"\"Function to get answer from a single context\"\"\"\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    confidence = float(outputs.start_logits[0, answer_start] + outputs.end_logits[0, answer_end])\n",
    "    answer_tokens = inputs.input_ids[0, answer_start: answer_end + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    return answer, confidence\n",
    "\n",
    "# Get answer using sliding window\n",
    "best_answer, best_score, best_window = get_answer_sliding_window(question, long_context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Best Answer: {best_answer}\")\n",
    "print(f\"From Window: {best_window[:100]}...\")\n",
    "print(f\"Confidence Score: {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
